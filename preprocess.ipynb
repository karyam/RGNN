{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pXDXI4KNP_3RiFzMByyVTEyTLpi3oNWm",
      "authorship_tag": "ABX9TyM8RarDq/jqzqSEkVmrqtDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karyam/rgnn_eeg_emotion_classifier/blob/main/preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHmn8zTpJrr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "import tensorboard\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as io\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxFzsE95pR48"
      },
      "source": [
        "#globals\n",
        "FEATURES_DATA_PATH = \"drive/My Drive/BCI/code/data/SEED/ExtractedFeatures\"\n",
        "PREPROCESSED_DATA_PATH = \"drive/My Drive/BCI/code/data/SEED/Preprocessed_EEG\"\n",
        "CSV_PATH = \"drive/My Drive/BCI_clone/CSV\"\n",
        "NPY_PATH = \"drive/My Drive/BCI_clone/npy\"\n",
        "SUBJECT_CSV_PATH = CSV_PATH + \"/\" + \"csv_0.csv\"\n",
        "ALL_SUBJECTS_CSV_PATH = CSV_PATH + \"/\" + \"csv_all_subjects.csv\"\n",
        "\n",
        "# sampling frequency\n",
        "sf = 200\n",
        "\n",
        "# hyper-parameters\n",
        "num_trials = 15\n",
        "num_subjects = 15\n",
        "num_bands = 5\n",
        "num_classes = 3\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6166y69sqWPX"
      },
      "source": [
        "#### Examine data format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31cMuKjhph-u",
        "outputId": "b6005744-ba29-44f0-aa19-4ba673a4a2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# get only subject data and sort for convenience\n",
        "data = os.listdir(FEATURES_DATA_PATH)\n",
        "# only take subject files\n",
        "data = [x for x in data if len(x.split(\"_\")) == 2] \n",
        "data.sort(key = lambda x : int(x.split(\"_\")[0]))\n",
        "# 3 files per subject, each file contains recordings for 15 trials\n",
        "assert (len(data) == 45)\n",
        "\n",
        "# load one sample \n",
        "sample = io.loadmat(os.path.join(FEATURES_DATA_PATH, data[0]))\n",
        "\n",
        "keys = list(sample.keys())\n",
        "assert (len(keys) == (2*6*15+3)) # 3 meta keys\n",
        "print(\"One sample shape: (num_channels, num_windows, num_bands)\")\n",
        "print(sample[\"de_LDS1\"].shape)\n",
        "\n",
        "# get all features averaged with LDS\n",
        "features_LDS = keys[4::2]\n",
        "print(\"Feature names LDS averaged\")\n",
        "print(features_LDS)\n",
        "assert (len(features_LDS) == (15*6))\n",
        "\n",
        "labels = io.loadmat(os.path.join(FEATURES_DATA_PATH, \"label.mat\"))\n",
        "labels = np.squeeze(labels[\"label\"] + np.ones(15, dtype=np.int8))\n",
        "assert (labels.shape == (15,))\n",
        "print(type(labels[0]))\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One sample shape: (num_channels, num_windows, num_bands)\n",
            "(62, 235, 5)\n",
            "Feature names LDS averaged\n",
            "['de_LDS1', 'psd_LDS1', 'dasm_LDS1', 'rasm_LDS1', 'asm_LDS1', 'dcau_LDS1', 'de_LDS2', 'psd_LDS2', 'dasm_LDS2', 'rasm_LDS2', 'asm_LDS2', 'dcau_LDS2', 'de_LDS3', 'psd_LDS3', 'dasm_LDS3', 'rasm_LDS3', 'asm_LDS3', 'dcau_LDS3', 'de_LDS4', 'psd_LDS4', 'dasm_LDS4', 'rasm_LDS4', 'asm_LDS4', 'dcau_LDS4', 'de_LDS5', 'psd_LDS5', 'dasm_LDS5', 'rasm_LDS5', 'asm_LDS5', 'dcau_LDS5', 'de_LDS6', 'psd_LDS6', 'dasm_LDS6', 'rasm_LDS6', 'asm_LDS6', 'dcau_LDS6', 'de_LDS7', 'psd_LDS7', 'dasm_LDS7', 'rasm_LDS7', 'asm_LDS7', 'dcau_LDS7', 'de_LDS8', 'psd_LDS8', 'dasm_LDS8', 'rasm_LDS8', 'asm_LDS8', 'dcau_LDS8', 'de_LDS9', 'psd_LDS9', 'dasm_LDS9', 'rasm_LDS9', 'asm_LDS9', 'dcau_LDS9', 'de_LDS10', 'psd_LDS10', 'dasm_LDS10', 'rasm_LDS10', 'asm_LDS10', 'dcau_LDS10', 'de_LDS11', 'psd_LDS11', 'dasm_LDS11', 'rasm_LDS11', 'asm_LDS11', 'dcau_LDS11', 'de_LDS12', 'psd_LDS12', 'dasm_LDS12', 'rasm_LDS12', 'asm_LDS12', 'dcau_LDS12', 'de_LDS13', 'psd_LDS13', 'dasm_LDS13', 'rasm_LDS13', 'asm_LDS13', 'dcau_LDS13', 'de_LDS14', 'psd_LDS14', 'dasm_LDS14', 'rasm_LDS14', 'asm_LDS14', 'dcau_LDS14', 'de_LDS15', 'psd_LDS15', 'dasm_LDS15', 'rasm_LDS15', 'asm_LDS15', 'dcau_LDS15']\n",
            "<class 'numpy.int16'>\n",
            "[2 1 0 0 1 2 0 1 2 2 1 0 1 2 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3keDbUYmpmAP",
        "outputId": "9a927155-26ca-4716-a1de-6b772c916d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the range of values across samples from de_LDS feature\n",
        "max_value = -1e18\n",
        "min_value = 1e18\n",
        "de_max_value = -1e18\n",
        "de_min_value = 1e18\n",
        "\n",
        "sample = io.loadmat(os.path.join(FEATURES_DATA_PATH, data[0]))\n",
        "# get all the de_lds feature keys since for the final model I will only use de_lds features\n",
        "de_keys = [key for key in sample.keys() if \"de_LDS\" in key]\n",
        "assert (len(de_keys) == 15)\n",
        "\n",
        "for sample in data:\n",
        "  sample = io.loadmat(os.path.join(FEATURES_DATA_PATH, sample))\n",
        "  for key in features_LDS:\n",
        "    if key in de_keys:\n",
        "      de_max_value = max(de_max_value, np.amax(sample[key]))\n",
        "      de_min_value = min(de_min_value, np.amin(sample[key]))\n",
        "    max_value = max(max_value, np.amax(sample[key]))\n",
        "    min_value = min(min_value, np.amin(sample[key]))\n",
        "print((min_value, max_value))\n",
        "print(f'de range: {(de_min_value, de_max_value)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(-23.825413747036478, 1072646499245.6045)\n",
            "de range: (10.567626836074302, 42.11366999020901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivkkaf3hp-7B"
      },
      "source": [
        "#### Reformat to csv files for convenience and using tfdv "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHFP9NZdp2Ql"
      },
      "source": [
        "# dataframe format: each row represents one data sample (one window from trial)\n",
        "# columns: all the provided features with LDS averaging for each channel and for each band\n",
        "features=['de_LDS','psd_LDS','dasm_LDS','rasm_LDS','asm_LDS','dcau_LDS']\n",
        "num_ch_per_feature = [62, 62, 27, 27, 54, 23]\n",
        "bands = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
        "columns = []\n",
        "count = 0\n",
        "\n",
        "for i, f in enumerate(features):\n",
        "  for ch in range(1, num_ch_per_feature[i]+1):\n",
        "    for b in bands:\n",
        "      columns.append(str(f + \"_\" + str(ch) + \"_\" + b))\n",
        "\n",
        " \n",
        "# add column for labels\n",
        "columns.append(\"label\")\n",
        "assert (len(columns) == (sum(i*5 for i in num_ch_per_feature)+1))\n",
        "\n",
        "# number of features 62*5 + 62*5 + 27*5 + 27*5 + 54*5 + 23*5 = 1275.\n",
        "total_num_features = len(columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0XZa6GeqG_i"
      },
      "source": [
        "def get_data_from_file(file:str):\n",
        "  x = io.loadmat(os.path.join(FEATURES_DATA_PATH, file))\n",
        "  trial_data = [0] * 15\n",
        "  total_num_wind = 0\n",
        "\n",
        "  for i in range(0,90,6): # process each trial data: 90 = 15*6\n",
        "    trial = int(i//6) #0-indexed\n",
        "    f = [0] * 6\n",
        "    feature_name = features_LDS[i]\n",
        "    assert (feature_name == f\"de_LDS{trial+1}\")\n",
        "    num_wind_trial = x[feature_name].shape[1]\n",
        "    total_num_wind += num_wind_trial\n",
        "  \n",
        "    # process the entire set of 6 features\n",
        "    for j in range(6):\n",
        "      f[j] = x[features_LDS[i+j]]\n",
        "      f[j] = np.swapaxes(f[j],0,1) # swap the ch and num_wind axis\n",
        "      # concatenate all data points across channels\n",
        "      f[j] = np.reshape(f[j], (-1, num_ch_per_feature[j]*num_bands))\n",
        "      assert (f[j].shape == (num_wind_trial, 5*num_ch_per_feature[j]))\n",
        "\n",
        "      if j == 0: # de feature\n",
        "        assert (np.amax(f[j]) <= de_max_value)\n",
        "        assert (np.amin(f[j]) >= de_min_value)\n",
        "\n",
        "    # assign to each window the corresponding trial label\n",
        "    l = list([labels[trial]] * num_wind_trial)\n",
        "    trial_labels = np.array([l])\n",
        "    trial_labels = np.reshape(trial_labels, (num_wind_trial, -1))\n",
        "    assert (np.unique(trial_labels).shape == (1,))\n",
        "    assert (trial_labels[0] == labels[trial])\n",
        "\n",
        "    # concatenate features across the horizontal axis: # feature_ch_band order + an additional column for labels\n",
        "    trial_data[trial] = np.concatenate([f[idx] for idx in range(6)] + [trial_labels], 1) \n",
        "    assert (trial_data[trial].shape == (num_wind_trial, total_num_features))\n",
        "  \n",
        "  entire_file_data = np.concatenate([trial_data[idx] for idx in range(num_trials)], axis=0)\n",
        "  assert entire_file_data.shape == (total_num_wind, total_num_features)\n",
        "  assert (np.amax(entire_file_data) <= max_value)\n",
        "  assert (np.amin(entire_file_data) >= min_value)\n",
        "  \n",
        "  return entire_file_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYYPUroFqKGK"
      },
      "source": [
        "def get_all_data_for_subject(file1, file2, file3):\n",
        "  series1 = get_data_from_file(file1)\n",
        "  series2 = get_data_from_file(file2)\n",
        "  series3 = get_data_from_file(file3)\n",
        "  series = np.concatenate([series1, series2, series3], axis=0)\n",
        "  assert (series.shape == (3*series1.shape[0], total_num_features))\n",
        "  assert (np.amax(series) <= max_value)\n",
        "  assert (np.amin(series) >= min_value)\n",
        "  return series"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps-zaqC_qNML"
      },
      "source": [
        "all_dfs = []\n",
        "for i in range(0,45,3):\n",
        "  # get all data for each subject\n",
        "  subject_data = get_all_data_for_subject(data[i], data[i+1], data[i+2])\n",
        "  #convert numpy data to DataFrame\n",
        "  data_dict = {}\n",
        "  for j, col in enumerate(columns):\n",
        "    data_dict[col] = subject_data[:, j]\n",
        "  df = pd.DataFrame(data_dict)\n",
        "  all_dfs.append(df)\n",
        "  #save dataframe to csv\n",
        "  df.to_csv(CSV_PATH + \"/\" + \"csv_\" + str((i+1)//3) + \".csv\")\n",
        "  \n",
        "all_subjects_df = pd.concat(all_dfs)\n",
        "all_subjects_df.to_csv(CSV_PATH + \"/\" + \"csv_all_subjects.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2WfWW9E8KKK"
      },
      "source": [
        "#### Concatenate more windows in one training sample\n",
        "*The above blocks treat a data sample as a single 1s window, however in order to effectively classify the input more time steps (1s windows) need to be taken into consideration.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLh7UxTiJ2Oh"
      },
      "source": [
        "def get_wind_from_file(file:str, w_len, drop_incomplete=True):\n",
        "  x = io.loadmat(os.path.join(FEATURES_DATA_PATH, file))\n",
        "  all_data, all_labels = [], []\n",
        "  total_num_wind = 0\n",
        "\n",
        "  for i in range(0,90,6):\n",
        "    trial = int(i//6) #0-indexed\n",
        "    trial_data = []\n",
        "\n",
        "    # take only de_lds feature\n",
        "    assert (features_LDS[i] == f\"de_LDS{trial+1}\")\n",
        "    f = x[features_LDS[i]]\n",
        "    num_wind_trial = f.shape[1]\n",
        "    \n",
        "    for j in range(0, num_wind_trial, w_len): # concat w_len samples\n",
        "      if drop_incomplete is True and (j + w_len > num_wind_trial): break;\n",
        "      window = f[:, j:j+w_len, :]\n",
        "      assert (window.shape == (62, w_len, 5))\n",
        "      window = np.reshape(window, (62, -1))\n",
        "      assert (window.shape == (62, w_len*5))\n",
        "      trial_data.append(window)\n",
        "    \n",
        "    trial_data = np.stack(trial_data, axis=0)\n",
        "    num_wind_trial = trial_data.shape[0]\n",
        "    total_num_wind += num_wind_trial\n",
        "\n",
        "    assert (trial_data.shape == (num_wind_trial, 62, w_len*num_bands))\n",
        "    assert (np.amax(trial_data) <= de_max_value)\n",
        "    assert (np.amin(trial_data) >= de_min_value)\n",
        "\n",
        "    # assign to each window the corresponding trial label\n",
        "    trial_labels = np.array(list([labels[trial]] * trial_data.shape[0]))\n",
        "    assert (np.unique(trial_labels).shape == (1,))\n",
        "    assert (trial_labels[0] == labels[trial])\n",
        "\n",
        "    all_data.append(trial_data)\n",
        "    all_labels.append(trial_labels)\n",
        "    \n",
        "  all_data = np.concatenate(all_data, axis=0)\n",
        "  all_labels = np.concatenate(all_labels, axis=0)\n",
        "  assert (all_data.shape == (total_num_wind, 62, num_bands*w_len))\n",
        "  assert (all_labels.shape == (total_num_wind,))\n",
        "  assert (np.amax(all_data) <= de_max_value)\n",
        "  assert (np.amin(all_data) >= de_min_value)\n",
        "  return all_data, all_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJVgwy44EvRg"
      },
      "source": [
        "def get_wind_for_subject(file1, file2, file3, w_len):\n",
        "  series1, l1 = get_wind_from_file(file1, w_len)\n",
        "  series2, l2 = get_wind_from_file(file2, w_len)\n",
        "  series3, l3 = get_wind_from_file(file3, w_len)\n",
        "\n",
        "  series = np.concatenate([series1, series2, series3], axis=0)\n",
        "  l = np.concatenate([l1, l2, l3], axis=0)\n",
        "  assert (series.shape == (3*series1.shape[0], 62, w_len*num_bands))\n",
        "  assert (l.shape == (3*series1.shape[0],))\n",
        "  assert (np.amax(series) <= de_max_value)\n",
        "  assert (np.amin(series) >= de_min_value)\n",
        "  return series, l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOtan698Zor"
      },
      "source": [
        "def get_more_windows_data(w_len):\n",
        "  all_data, all_labels = [], []\n",
        "  for i in range(0,45,3):\n",
        "    subject_data, subject_labels = get_wind_for_subject(data[i], data[i+1], data[i+2], w_len)\n",
        "    with open(NPY_PATH + \"/\" + \"npy_\" + str((i+1)//3) + \".npy\", 'wb') as f:\n",
        "      np.save(f, subject_data)\n",
        "    with open(NPY_PATH + \"/\" + \"npy_\" + str((i+1)//3) + \"_label.npy\", 'wb') as f:\n",
        "      np.save(f, subject_labels)\n",
        "    all_data.append(subject_data)\n",
        "    all_labels.append(subject_labels)\n",
        "  \n",
        "  all_data = np.concatenate(all_data, axis=0)\n",
        "  all_labels = np.concatenate(all_labels, axis=0)\n",
        "  assert (all_data.shape[0] == all_labels.shape[0])\n",
        "  with open(NPY_PATH + \"/\" + \"npy_all_subjects.npy\", 'wb') as f:\n",
        "      np.save(f, all_data)\n",
        "  with open(NPY_PATH + \"/\" + \"npy_all_subjects_label.npy\", 'wb') as f:\n",
        "      np.save(f, all_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjM8zvTDd0Z5"
      },
      "source": [
        "get_more_windows_data(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}