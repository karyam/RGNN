{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karyam/rgnn_eeg_emotion_classifier/blob/main/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0vfbPSltBdi"
      },
      "source": [
        "!pip install dgl\n",
        "!pip install -U mne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hylbGi-eYZHr"
      },
      "source": [
        "import tensorflow as tf\n",
        "print('Installing TensorFlow Data Validation')\n",
        "!pip install -q tensorflow_data_validation[visualization]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxCLiYnvmlw4",
        "outputId": "3b490006-a3a8-4977-9af1-42daf7912245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install tfdlpack"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfdlpack\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/24/2812b0e28c192c05581d9e8143d55d35db55c27490337e935f16b1278f83/tfdlpack-0.1.3-cp36-cp36m-manylinux1_x86_64.whl (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hInstalling collected packages: tfdlpack\n",
            "Successfully installed tfdlpack-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll4W3x9tuTQp",
        "outputId": "a9ed9094-1915-457b-e5ae-2b5e3ac53043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# sometimes this cell needs to be run multiple times idk why\n",
        "!python -m dgl.backend.set_default_backend tensorflow\n",
        "#!python -m dgl.backend.set_default_backend pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using backend: tensorflow\n",
            "2020-10-13 11:51:10.553426: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-10-13 11:51:12.480573: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-10-13 11:51:12.545970: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-10-13 11:51:12.546045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (57bf1ddcfed0): /proc/driver/nvidia/version does not exist\n",
            "2020-10-13 11:51:12.591276: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-10-13 11:51:12.591571: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27aea00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-10-13 11:51:12.591616: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'dgl.backend.set_default_backend' found in sys.modules after import of package 'dgl.backend', but prior to execution of 'dgl.backend.set_default_backend'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "Setting the default backend to \"tensorflow\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpc0mKrzsW3g",
        "outputId": "7879ae7b-8d8a-4109-cd7f-3837d33e243f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXqtOdYHbBj5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorboard\n",
        "import dgl\n",
        "import scipy.io as io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dgl.data.utils import *\n",
        "from dgl.nn import GraphConv\n",
        "import tensorflow_data_validation as tfdv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CNCnhURxbU1"
      },
      "source": [
        "#globals\n",
        "FEATURES_DATA_PATH = \"drive/My Drive/BCI/code/data/SEED/ExtractedFeatures\"\n",
        "PREPROCESSED_DATA_PATH = \"drive/My Drive/BCI/code/data/SEED/Preprocessed_EEG\"\n",
        "CSV_PATH = \"drive/My Drive/BCI_clone/CSV\"\n",
        "SUBJECT_NPY_DATA_PATH = \"drive/My Drive/BCI_clone/npy/npy_0.npy\"\n",
        "SUBJECT_NPY_LABEL_PATH = \"drive/My Drive/BCI_clone/npy/npy_0_label.npy\"\n",
        "SUBJECT_CSV_PATH = CSV_PATH + \"/\" + \"csv_0.csv\"\n",
        "ALL_SUBJECTS_CSV_PATH = CSV_PATH + \"/\" + \"csv_all_subjects.csv\"\n",
        "ALL_SUBJECTS_NPY_DATA_PATH = \"drive/My Drive/BCI_clone/npy/npy_all_subjects.npy\"\n",
        "ALL_SUBJECTS_NPY_LABEL_PATH = \"drive/My Drive/BCI_clone/npy/npy_all_subjects_label.npy\"\n",
        "GRAPH_PATH = \"drive/My Drive/BCI_clone/graph/\"\n",
        "CHECKPOINTS_PATH = \"drive/My Drive/BCI_clone/checkpoints/\"\n",
        "\n",
        "# sampling frequency\n",
        "sf = 200\n",
        "\n",
        "# hyper-parameters\n",
        "num_trials = 15\n",
        "num_subjects = 15\n",
        "num_bands = 5\n",
        "num_classes = 3"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0iuNpvAYpm8"
      },
      "source": [
        "# get stats per subject using tfdv\n",
        "subject_stats = tfdv.generate_statistics_from_csv(data_location=SUBJECT_CSV_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGlrk4XZjNi"
      },
      "source": [
        "tfdv.visualize_statistics(subject_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zPJ9afM8dXY"
      },
      "source": [
        "#### Analyse feature correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukPXWjPkeBYp"
      },
      "source": [
        "subject_df = pd.read_csv(SUBJECT_CSV_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3M_fnXSc_Q4"
      },
      "source": [
        "# get correlated features\n",
        "corr_features = set()\n",
        "corr_matrix = subject_df.corr()\n",
        "# visualization doesn't help since there are way to many features\n",
        "# plt.figure(figsize=(100,100))\n",
        "# sns.heatmap(corr_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfWTvEHvh2_G"
      },
      "source": [
        "# get features most correlated with the label\n",
        "# bands: gamma and beta show meaningful corelation with EEG-based emotion classification\n",
        "# as specified by https://www.researchgate.net/publication/276443876_Investigating_Critical_Frequency_Bands_and_Channels_for_EEG-Based_Emotion_Recognition_with_Deep_Neural_Networks\n",
        "pos_corr_features = corr_matrix.sort_values(by=[\"label\"], ascending=False)[\"label\"].head()\n",
        "neg_corr_features = corr_matrix.sort_values(by=[\"label\"])[\"label\"].head()\n",
        "\n",
        "print(pos_corr_features)\n",
        "print()\n",
        "print(neg_corr_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6TAXtzfJOM"
      },
      "source": [
        "for i in range(len(subject_df.columns)):\n",
        "  for j in range(i):\n",
        "    if abs(corr_matrix.iloc[i,j]) > 0.8 and subject_df.columns[i] != \"label\" and subject_df.columns[j] not in pos_corr_features + neg_corr_features: \n",
        "      corr_features.add(corr_matrix.columns[i])\n",
        "\n",
        "print(len(corr_features))\n",
        "subject_df.drop(labels=corr_features, axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBg1qMXgl173"
      },
      "source": [
        "len(subject_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kzuz8ofTll_"
      },
      "source": [
        "def get_de_lds_from_csv(df):\n",
        "  X = df.sample(frac=1).reset_index(drop=True) # shuffle dataset\n",
        "  Y = X[\"label\"].to_numpy()\n",
        "  X.drop(labels=\"label\", axis=1, inplace=True)\n",
        "  X.drop(labels=df.columns[0], axis=1, inplace=True)  # drop the index column\n",
        "  X.drop(labels=df.columns[62*5+1:-1], axis=1, inplace=True) # take only de_lds features\n",
        "  X = X.to_numpy()\n",
        "  return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRnYJfo_DsOP"
      },
      "source": [
        "### Baseline classification (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_HrWzaYu4UU"
      },
      "source": [
        "#### Subject-dependent training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZJ1xXLqsNOl"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(16, activation=\"relu\"),\n",
        "    keras.layers.Dense(16, activation=\"relu\"),\n",
        "    keras.layers.Dense(num_classes)\n",
        "])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFVXeCfXufOm"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m-L53otqhzB",
        "outputId": "a8c0addf-b11a-4a9c-9fb6-384fed277378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "X = np.load(SUBJECT_NPY_DATA_PATH)\n",
        "Y = np.load(SUBJECT_NPY_LABEL_PATH)\n",
        "X = np.reshape(X, (-1, 62*25))\n",
        "print(X.shape, Y.shape)\n",
        "# do not forget to shuffle samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2025, 1550) (2025,)\n",
            "(1620, 1550) (1620,)\n",
            "(405, 1550) (405,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY4MOVnbu9q5",
        "outputId": "1038df5f-db81-4a45-d0fa-7dc4404b41f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7858\n",
            "Epoch 2/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7438\n",
            "Epoch 3/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7907\n",
            "Epoch 4/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7809\n",
            "Epoch 5/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.8043\n",
            "Epoch 6/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.7969\n",
            "Epoch 7/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.6260 - accuracy: 0.7284\n",
            "Epoch 8/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7975\n",
            "Epoch 9/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8074\n",
            "Epoch 10/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8272\n",
            "Epoch 11/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.7099\n",
            "Epoch 12/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8130\n",
            "Epoch 13/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7710\n",
            "Epoch 14/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8037\n",
            "Epoch 15/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.8019\n",
            "Epoch 16/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8414\n",
            "Epoch 17/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7562\n",
            "Epoch 18/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8216\n",
            "Epoch 19/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7772\n",
            "Epoch 20/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8278\n",
            "Epoch 21/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8210\n",
            "Epoch 22/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7833\n",
            "Epoch 23/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8204\n",
            "Epoch 24/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8123\n",
            "Epoch 25/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7858\n",
            "Epoch 26/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8105\n",
            "Epoch 27/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4811 - accuracy: 0.7870\n",
            "Epoch 28/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.8136\n",
            "Epoch 29/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8272\n",
            "Epoch 30/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7710\n",
            "Epoch 31/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.7747\n",
            "Epoch 32/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.8086\n",
            "Epoch 33/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8481\n",
            "Epoch 34/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.8136\n",
            "Epoch 35/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4937 - accuracy: 0.7821\n",
            "Epoch 36/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8247\n",
            "Epoch 37/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.7741\n",
            "Epoch 38/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6391 - accuracy: 0.7006\n",
            "Epoch 39/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7451\n",
            "Epoch 40/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5559 - accuracy: 0.7265\n",
            "Epoch 41/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.8062\n",
            "Epoch 42/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.7179\n",
            "Epoch 43/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.8105\n",
            "Epoch 44/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7784\n",
            "Epoch 45/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.8807 - accuracy: 0.6444\n",
            "Epoch 46/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.6549\n",
            "Epoch 47/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7889\n",
            "Epoch 48/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7623\n",
            "Epoch 49/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.8056\n",
            "Epoch 50/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5197 - accuracy: 0.7765\n",
            "Epoch 51/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7710\n",
            "Epoch 52/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.6242 - accuracy: 0.7173\n",
            "Epoch 53/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.8031\n",
            "Epoch 54/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8259\n",
            "Epoch 55/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7802\n",
            "Epoch 56/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5285 - accuracy: 0.7537\n",
            "Epoch 57/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.8019\n",
            "Epoch 58/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.8012\n",
            "Epoch 59/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.6142 - accuracy: 0.7346\n",
            "Epoch 60/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7710\n",
            "Epoch 61/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.8080\n",
            "Epoch 62/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8037\n",
            "Epoch 63/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8216\n",
            "Epoch 64/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.7080\n",
            "Epoch 65/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.7901\n",
            "Epoch 66/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8414\n",
            "Epoch 67/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7735\n",
            "Epoch 68/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7852\n",
            "Epoch 69/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8204\n",
            "Epoch 70/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.7907\n",
            "Epoch 71/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.8019\n",
            "Epoch 72/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7358\n",
            "Epoch 73/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8370\n",
            "Epoch 74/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.8056\n",
            "Epoch 75/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8469\n",
            "Epoch 76/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7852\n",
            "Epoch 77/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4365 - accuracy: 0.8148\n",
            "Epoch 78/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8481\n",
            "Epoch 79/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8148\n",
            "Epoch 80/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.8204\n",
            "Epoch 81/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8173\n",
            "Epoch 82/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8494\n",
            "Epoch 83/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7784\n",
            "Epoch 84/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8352\n",
            "Epoch 85/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8457\n",
            "Epoch 86/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8383\n",
            "Epoch 87/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8549\n",
            "Epoch 88/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8315\n",
            "Epoch 89/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7938\n",
            "Epoch 90/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8093\n",
            "Epoch 91/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8426\n",
            "Epoch 92/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7784\n",
            "Epoch 93/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.8525\n",
            "Epoch 94/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8451\n",
            "Epoch 95/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7654\n",
            "Epoch 96/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8444\n",
            "Epoch 97/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7802\n",
            "Epoch 98/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.6138 - accuracy: 0.7191\n",
            "Epoch 99/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8321\n",
            "Epoch 100/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8475\n",
            "Epoch 101/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8105\n",
            "Epoch 102/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8438\n",
            "Epoch 103/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8630\n",
            "Epoch 104/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8167\n",
            "Epoch 105/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8148\n",
            "Epoch 106/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8586\n",
            "Epoch 107/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4945 - accuracy: 0.7994\n",
            "Epoch 108/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6653 - accuracy: 0.7056\n",
            "Epoch 109/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4193 - accuracy: 0.8302\n",
            "Epoch 110/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8599\n",
            "Epoch 111/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5145 - accuracy: 0.7772\n",
            "Epoch 112/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8463\n",
            "Epoch 113/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8747\n",
            "Epoch 114/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7735\n",
            "Epoch 115/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3997 - accuracy: 0.8241\n",
            "Epoch 116/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8191\n",
            "Epoch 117/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8296\n",
            "Epoch 118/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8370\n",
            "Epoch 119/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8309\n",
            "Epoch 120/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8593\n",
            "Epoch 121/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.7363 - accuracy: 0.7006\n",
            "Epoch 122/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8179\n",
            "Epoch 123/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8704\n",
            "Epoch 124/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8179\n",
            "Epoch 125/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8309\n",
            "Epoch 126/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7679\n",
            "Epoch 127/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8414\n",
            "Epoch 128/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8636\n",
            "Epoch 129/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7920\n",
            "Epoch 130/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8481\n",
            "Epoch 131/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8605\n",
            "Epoch 132/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7636\n",
            "Epoch 133/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5717 - accuracy: 0.7660\n",
            "Epoch 134/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8395\n",
            "Epoch 135/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8352\n",
            "Epoch 136/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8586\n",
            "Epoch 137/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8370\n",
            "Epoch 138/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8469\n",
            "Epoch 139/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8772\n",
            "Epoch 140/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8302\n",
            "Epoch 141/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7784\n",
            "Epoch 142/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8562\n",
            "Epoch 143/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8364\n",
            "Epoch 144/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8796\n",
            "Epoch 145/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.7364\n",
            "Epoch 146/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8716\n",
            "Epoch 147/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8568\n",
            "Epoch 148/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8025\n",
            "Epoch 149/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8117\n",
            "Epoch 150/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8889\n",
            "Epoch 151/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8512\n",
            "Epoch 152/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3013 - accuracy: 0.8716\n",
            "Epoch 153/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8506\n",
            "Epoch 154/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8568\n",
            "Epoch 155/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8278\n",
            "Epoch 156/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5668 - accuracy: 0.7660\n",
            "Epoch 157/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8691\n",
            "Epoch 158/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8420\n",
            "Epoch 159/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.7623\n",
            "Epoch 160/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8741\n",
            "Epoch 161/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3156 - accuracy: 0.8741\n",
            "Epoch 162/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8358\n",
            "Epoch 163/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8574\n",
            "Epoch 164/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8660\n",
            "Epoch 165/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3894 - accuracy: 0.8420\n",
            "Epoch 166/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8259\n",
            "Epoch 167/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8648\n",
            "Epoch 168/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8815\n",
            "Epoch 169/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7963\n",
            "Epoch 170/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4145 - accuracy: 0.8093\n",
            "Epoch 171/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8747\n",
            "Epoch 172/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8796\n",
            "Epoch 173/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8827\n",
            "Epoch 174/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4928 - accuracy: 0.8117\n",
            "Epoch 175/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3976 - accuracy: 0.8235\n",
            "Epoch 176/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8364\n",
            "Epoch 177/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8654\n",
            "Epoch 178/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8475\n",
            "Epoch 179/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.8093\n",
            "Epoch 180/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4135 - accuracy: 0.8228\n",
            "Epoch 181/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8599\n",
            "Epoch 182/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8642\n",
            "Epoch 183/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8809\n",
            "Epoch 184/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.5780 - accuracy: 0.7543\n",
            "Epoch 185/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.7969\n",
            "Epoch 186/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8556\n",
            "Epoch 187/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8895\n",
            "Epoch 188/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8790\n",
            "Epoch 189/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8451\n",
            "Epoch 190/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8840\n",
            "Epoch 191/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.4037 - accuracy: 0.8222\n",
            "Epoch 192/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8531\n",
            "Epoch 193/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3074 - accuracy: 0.8747\n",
            "Epoch 194/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8840\n",
            "Epoch 195/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8722\n",
            "Epoch 196/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8648\n",
            "Epoch 197/200\n",
            "51/51 [==============================] - 0s 2ms/step - loss: 0.2835 - accuracy: 0.8907\n",
            "Epoch 198/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8469\n",
            "Epoch 199/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8790\n",
            "Epoch 200/200\n",
            "51/51 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yrLdm6xvqjn",
        "outputId": "261305a1-4a28-431a-972f-17d64ee36692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#evaluate subject-dependent mlp: accuracy ~90%\n",
        "history = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGylW6Vutke"
      },
      "source": [
        "#### Subject-independent training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrpkeMRMuyP3"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(64, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(16, activation=\"relu\"),\n",
        "    keras.layers.Dense(16, activation=\"relu\"),\n",
        "    keras.layers.Dense(num_classes)\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_SVC_8DOMat"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Q75KRCNx63",
        "outputId": "3501b435-cd6f-457f-b4ef-303c4ef1f30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = np.load(ALL_SUBJECTS_NPY_DATA_PATH)\n",
        "Y = np.load(ALL_SUBJECTS_NPY_LABEL_PATH)\n",
        "print(X.shape, Y.shape)\n",
        "X = np.reshape(X, (-1, 62*25))\n",
        "print(X.shape, Y.shape)\n",
        "# do not forget to shuffle samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30375, 62, 25) (30375,)\n",
            "(30375, 1550) (30375,)\n",
            "(24300, 1550) (24300,)\n",
            "(6075, 1550) (6075,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MaCsO1_PWAU",
        "outputId": "359476ef-1ae3-4b06-bce8-18afd792c383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.1626 - accuracy: 0.3775\n",
            "Epoch 2/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0239 - accuracy: 0.4558\n",
            "Epoch 3/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 0.9595 - accuracy: 0.4921\n",
            "Epoch 4/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 0.9510 - accuracy: 0.5044\n",
            "Epoch 5/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 0.9261 - accuracy: 0.5185\n",
            "Epoch 6/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 0.9038 - accuracy: 0.5287\n",
            "Epoch 7/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 0.8814 - accuracy: 0.5474\n",
            "Epoch 8/200\n",
            "760/760 [==============================] - 3s 3ms/step - loss: 0.8931 - accuracy: 0.5360\n",
            "Epoch 9/200\n",
            "760/760 [==============================] - 4s 5ms/step - loss: 0.8688 - accuracy: 0.5514\n",
            "Epoch 10/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 0.8573 - accuracy: 0.5590\n",
            "Epoch 11/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0872 - accuracy: 0.3525\n",
            "Epoch 12/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.3458\n",
            "Epoch 13/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.3456\n",
            "Epoch 14/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0986 - accuracy: 0.3456\n",
            "Epoch 15/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.3461\n",
            "Epoch 16/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.3445\n",
            "Epoch 17/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3427\n",
            "Epoch 18/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0985 - accuracy: 0.3470\n",
            "Epoch 19/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 20/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3453\n",
            "Epoch 21/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 22/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3462\n",
            "Epoch 23/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3467\n",
            "Epoch 24/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0985 - accuracy: 0.3470\n",
            "Epoch 25/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 26/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3465\n",
            "Epoch 27/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 28/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 29/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 30/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 31/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3457\n",
            "Epoch 32/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3460\n",
            "Epoch 33/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3456\n",
            "Epoch 34/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 35/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3454\n",
            "Epoch 36/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 37/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 38/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 39/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 40/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 41/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 42/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 43/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 44/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 45/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 46/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 47/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 48/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 49/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3458\n",
            "Epoch 50/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 51/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 52/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 53/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 54/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 55/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 56/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 57/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 58/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0982 - accuracy: 0.3459\n",
            "Epoch 59/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 60/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 61/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 62/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 63/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3460\n",
            "Epoch 64/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 65/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 66/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 67/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 68/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 69/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 70/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 71/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 72/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 73/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 74/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 75/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 76/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 77/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 78/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 79/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 80/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 81/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 82/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 83/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 84/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3460\n",
            "Epoch 85/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 86/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 87/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 88/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 89/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 90/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 91/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 92/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 93/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 94/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0984 - accuracy: 0.3470\n",
            "Epoch 95/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 96/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 97/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 98/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 99/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 100/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 101/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 102/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 103/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 104/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 105/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 106/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 107/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 108/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 109/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 110/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 111/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 112/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 113/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 114/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 115/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 116/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 117/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 118/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 119/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 120/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 121/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 122/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 123/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 124/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 125/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 126/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0982 - accuracy: 0.3470\n",
            "Epoch 127/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 128/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 129/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 130/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 131/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 132/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 133/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0982 - accuracy: 0.3470\n",
            "Epoch 134/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 135/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 136/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 137/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 138/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 139/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 140/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 141/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 142/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 143/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 144/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 145/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 146/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 147/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 148/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 149/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 150/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 151/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 152/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 153/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 154/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 155/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 156/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 157/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 158/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 159/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 160/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 161/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 162/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 163/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 164/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 165/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 166/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 167/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 168/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 169/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 170/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 171/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 172/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0982 - accuracy: 0.3470\n",
            "Epoch 173/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 174/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 175/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 176/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 177/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 178/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 179/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 180/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 181/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 182/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 183/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 184/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 185/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 186/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 187/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 188/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 189/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 190/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 191/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 192/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 193/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 194/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 195/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 196/200\n",
            "760/760 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 197/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 198/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 199/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n",
            "Epoch 200/200\n",
            "760/760 [==============================] - 2s 3ms/step - loss: 1.0983 - accuracy: 0.3470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T19EcFjVPYIq"
      },
      "source": [
        "#evaluate subject-independent mlp: accuracy ~35%\n",
        "history = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUbViu76qs4m"
      },
      "source": [
        "### Baseline Graph Convolution Network\n",
        "\n",
        "* **Input** - DGL Graph object\n",
        "* **Output** - predicted logits per each graph\n",
        "* **Layers** - GC64M16 - GC64M16 - P2 - GC128M9 - GC128M9 - P2 - FC40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM7SFayXyhbV"
      },
      "source": [
        "#### Build input graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWZq84n0c8SA"
      },
      "source": [
        "#TODO: Test with multiple bi-variate metrics for edge features\n",
        "def correlation(signal1, signal2):\n",
        "  return abs(np.corrcoef(signal1, signal2)[0][1])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EconL9Fs2LF"
      },
      "source": [
        "def signal_2_graph(signal, treshold=0.6, channel_dist=correlation, num_channels=62):\n",
        "  \"\"\"\n",
        "  Function to create one graph data sample (window)\n",
        "  Arguments:\n",
        "    - signal (ndarray(num_ch, num_bands*num_features)): signal(features) from a single window\n",
        "    - treshold\n",
        "    - channel_dist: distance measure between 2 channels\n",
        "    - num_channels\n",
        "  Return:\n",
        "    - graph (DGLGraph object) \n",
        "  \"\"\"\n",
        "\n",
        "  src = []\n",
        "  dest = []\n",
        "  edgef = []\n",
        "\n",
        "  for i in range(num_channels-1):\n",
        "    for j in range(i+1,num_channels):\n",
        "      weight = channel_dist(signal[i], signal[j])\n",
        "      if weight > treshold:\n",
        "        src.append(i)\n",
        "        dest.append(j)\n",
        "        edgef.append(weight)\n",
        "        #dgl edges are in stored of their addition\n",
        "\n",
        "  #make edges bi-directional for undirected graph\n",
        "  u = np.concatenate([src, dest])\n",
        "  v = np.concatenate([dest, src])\n",
        "  graph = dgl.DGLGraph((u, v))\n",
        "  \n",
        "  assert graph.number_of_nodes() == num_channels \n",
        "  graph.add_edges(graph.nodes(), graph.nodes()) # add self-loops\n",
        "  \n",
        "  signal = tf.convert_to_tensor(signal, dtype='float32')\n",
        "  graph.ndata['x'] = signal\n",
        "  edgef = np.concatenate([edgef, edgef, [1]*num_channels])\n",
        "  \n",
        "  assert edgef.shape[0] == graph.number_of_edges()\n",
        "  \n",
        "  edgef = tf.convert_to_tensor(edgef, dtype='float32')\n",
        "  graph.edata['w'] = edgef\n",
        "  return graph"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWbvcMo3PQ_n"
      },
      "source": [
        "#TODO: Fix the labels dim mismatch problem\n",
        "def generate_subject_graphs(data, labels, subject):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "    - data: type(ndarray), shape(num_samples, num_channels, num_bands)\n",
        "  Return:\n",
        "    - graphs: type(ndarray of DGLGraph objects) shape(num_samples,)\n",
        "  \"\"\"\n",
        "  graphs = []\n",
        "  graph_labels = {\"glabels\": tf.convert_to_tensor(labels)}\n",
        "  num_samples = data.shape[0]\n",
        "  \n",
        "  for i in range(num_samples):\n",
        "    graphs.append(signal_2_graph(data[i]))\n",
        "  save_graphs(GRAPH_PATH+f'graphs_from_de_feature_subj_{subject}.bin', graphs, graph_labels)\n",
        "  return graphs"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcFEjb2SCqgs"
      },
      "source": [
        "X = np.load(SUBJECT_NPY_DATA_PATH)\n",
        "Y = np.load(SUBJECT_NPY_LABEL_PATH)\n",
        "graph_data = generate_subject_graphs(X, Y, subject=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEMT90SKLFFR",
        "outputId": "adf83db0-847a-45fc-deef-597dbb70f198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "X = np.load(ALL_SUBJECTS_NPY_DATA_PATH)\n",
        "Y = np.load(ALL_SUBJECTS_NPY_LABEL_PATH)\n",
        "graph_data = generate_subject_graphs(X, Y, subject=\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TMq3_tqtoRp"
      },
      "source": [
        "#### Create a batch of DGLGraphs\n",
        "*A batch of graphs can be viewed as a large graph that has many disjoint connected components*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_wNqSq8GsEk"
      },
      "source": [
        "def collate(graphs):\n",
        "  \"\"\"\n",
        "  Function to create a batched graph from a collection od DGLGraph objects.\n",
        "  Return:\n",
        "    - batched_graph: type(DGLGraph object)\n",
        "  \"\"\"\n",
        "  batched_graph = dgl.batch(graphs)\n",
        "  return batched_graph"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ancF_pJa6LU"
      },
      "source": [
        "def generate_graph_batches(graphs:list, labels, batch_size=10):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "    - graphs: type(list of DGLGraph objects),  shape(num_samples,)\n",
        "    - labels: type(list of per sample labels), shape(num_samples,)\n",
        "\n",
        "  Return:\n",
        "    - batches: type(ndarray of DGLGraph objects),  shape(num_samples//batch_size,) - array of batched graphs\n",
        "    - labels: type(ndarray), shape(num_samples//batch_size,)\n",
        "  \"\"\"\n",
        "\n",
        "  num_graphs = len(graphs)\n",
        "  num_batches=num_graphs//batch_size\n",
        "  # graphs = [graph.to('/gpu:0') for graph in graphs]\n",
        "\n",
        "  X_batches, y_batches = [], []\n",
        "  for i in range(0, num_graphs, batch_size):\n",
        "    if i+batch_size > num_graphs: break\n",
        "    X_batches.append(collate(graphs[i:i+batch_size]))\n",
        "    y_batches.append(labels[i:i+batch_size])\n",
        "  \n",
        "  assert len(X_batches) == num_batches\n",
        "  assert len(y_batches) == num_batches\n",
        "\n",
        "  return X_batches, y_batches"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j26yHCxlWVEB"
      },
      "source": [
        "def get_batched_train_data(batch_size=10, num_nodes=62, subject=0):\n",
        "  graph_data, graph_labels = load_graphs(os.path.join(GRAPH_PATH, f\"graphs_from_de_feature_subj{subject}.bin\"))\n",
        "  graph_labels = graph_labels['glabels'].numpy()\n",
        "  graph_labels = graph_labels.tolist()\n",
        "  assert (len(graph_data) == len(graph_labels))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(graph_data, graph_labels, test_size=0.2, random_state=42)\n",
        "  \n",
        "  X_train, y_train = generate_graph_batches(X_train, y_train)\n",
        "  X_test, y_test = generate_graph_batches(X_test, y_test)\n",
        "\n",
        "  for i, batched_graph in enumerate(X_train):\n",
        "    assert (batched_graph.number_of_nodes() == num_nodes * batch_size)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUg8oIzi-xE"
      },
      "source": [
        "X_train, X_test, y_train, y_test = get_batched_train_data()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4lSrXkLjsla",
        "outputId": "eea5b81b-9d28-40f1-c3ac-0cfa1068c69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(X_train), len(y_train))\n",
        "print(len(X_test), len(y_test))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162 162\n",
            "40 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zgv_klexR9H"
      },
      "source": [
        "class GCN(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               in_dim, \n",
        "               out_dim,\n",
        "               activation=tf.nn.relu, \n",
        "               dropout=0.5, \n",
        "               pooling=True, \n",
        "               hidden_dim=32):\n",
        "    \n",
        "    super(GCN, self).__init__()\n",
        "    self.in_dim = in_dim\n",
        "    self.glayers = []\n",
        "    self.glayers.append(GraphConv(in_dim, hidden_dim, activation=activation))\n",
        "    self.glayers.append(GraphConv(hidden_dim, hidden_dim, activation=activation))\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    self.logits = tf.keras.layers.Dense(out_dim)\n",
        "\n",
        "  def call(self, g):\n",
        "    h = g.ndata['x']\n",
        "    for i, layer in enumerate(self.glayers):\n",
        "      if i != 0:\n",
        "        h = self.dropout(h)\n",
        "      h = layer(g, h)\n",
        "\n",
        "    g.ndata['h'] = h\n",
        "    hg = dgl.mean_nodes(g, 'h')\n",
        "    return self.logits(hg)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0cT7sNcDTRD"
      },
      "source": [
        "class Trainer(object):\n",
        "  def __init__(self, loss_fcn, optimizer, num_epochs, model):\n",
        "    self.loss = loss_fcn\n",
        "    self.optimizer = optimizer\n",
        "    self.num_epochs = num_epochs\n",
        "    self.model = model\n",
        "    self.best_loss = None\n",
        "  \n",
        "  def eval(self, epoch, data):\n",
        "    X_test, y_test = data\n",
        "    eval_loss_avg = tf.keras.metrics.Mean()\n",
        "    eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    for X, y in zip(X_test, y_test):\n",
        "      logits = self.model(X)\n",
        "      loss_val = self.loss(y, logits)\n",
        "      eval_loss_avg.update_state(loss_val)\n",
        "      eval_accuracy.update_state(y, logits)\n",
        "    if self.best_loss is None or (eval_loss_avg.result() < self.best_loss):\n",
        "      self.model.save_weights(CHECKPOINTS_PATH)\n",
        "    print(\"Epoch {:03d}: Eval Loss: {:.3f}, Eval Accuracy: {:.3%}\".format(epoch, eval_loss_avg.result(), eval_accuracy.result()))\n",
        "\n",
        "  def run(self, data):\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "\n",
        "    X_train, y_train, X_test, y_test = data\n",
        "    for epoch in range(self.num_epochs):\n",
        "      epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "      epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "      \n",
        "      for X, y in zip(X_train, y_train):\n",
        "        with tf.GradientTape() as tape:\n",
        "          logits = self.model(X)\n",
        "          loss_val = self.loss(y, logits)\n",
        "          grads = tape.gradient(loss_val, model.trainable_weights)\n",
        "          self.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        epoch_loss_avg.update_state(loss_val)\n",
        "        epoch_accuracy.update_state(y, logits)\n",
        "\n",
        "      train_loss.append(epoch_loss_avg.result())\n",
        "      train_acc.append(epoch_accuracy.result())\n",
        "      if epoch % 20 == 0:\n",
        "        print(\"Epoch {:03d}: Train Loss: {:.3f}, Train Accuracy: {:.3%}\".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result()))\n",
        "        self.eval(epoch, (X_test, y_test))\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7wQP7RXndJm",
        "outputId": "cf86f3aa-3cb3-4de3-cbd6-b03da7d1fe70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# subject-dependent acc ~67%\n",
        "loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "num_features = 25\n",
        "model = GCN(in_dim=num_features, out_dim=num_classes)\n",
        "\n",
        "trainer = Trainer(loss_fcn, optimizer, num_epochs=200, model=model)\n",
        "trainer.run((X_train, y_train, X_test, y_test))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Train Loss: 2.437, Train Accuracy: 34.383%\n",
            "Epoch 000: Eval Loss: 1.176, Eval Accuracy: 31.500%\n",
            "Epoch 020: Train Loss: 1.007, Train Accuracy: 48.580%\n",
            "Epoch 020: Eval Loss: 0.939, Eval Accuracy: 57.000%\n",
            "Epoch 040: Train Loss: 0.927, Train Accuracy: 54.012%\n",
            "Epoch 040: Eval Loss: 0.888, Eval Accuracy: 60.500%\n",
            "Epoch 060: Train Loss: 0.860, Train Accuracy: 57.963%\n",
            "Epoch 060: Eval Loss: 0.811, Eval Accuracy: 61.750%\n",
            "Epoch 080: Train Loss: 0.828, Train Accuracy: 58.457%\n",
            "Epoch 080: Eval Loss: 0.798, Eval Accuracy: 61.250%\n",
            "Epoch 100: Train Loss: 0.796, Train Accuracy: 61.728%\n",
            "Epoch 100: Eval Loss: 0.740, Eval Accuracy: 64.750%\n",
            "Epoch 120: Train Loss: 0.787, Train Accuracy: 61.111%\n",
            "Epoch 120: Eval Loss: 0.773, Eval Accuracy: 62.000%\n",
            "Epoch 140: Train Loss: 0.771, Train Accuracy: 61.481%\n",
            "Epoch 140: Eval Loss: 0.772, Eval Accuracy: 63.750%\n",
            "Epoch 160: Train Loss: 0.763, Train Accuracy: 62.037%\n",
            "Epoch 160: Eval Loss: 0.730, Eval Accuracy: 66.750%\n",
            "Epoch 180: Train Loss: 0.766, Train Accuracy: 64.259%\n",
            "Epoch 180: Eval Loss: 0.705, Eval Accuracy: 67.750%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}