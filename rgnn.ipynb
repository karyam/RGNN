{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karyam/rgnn_eeg_emotion_classifier/blob/main/rgnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KMf8LZyLosW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l16-w2tILgRr"
      },
      "source": [
        "SUBJECT_NPY_DATA_PATH = \"drive/My Drive/BCI_clone/npy/npy_0.npy\"\n",
        "SUBJECT_NPY_LABEL_PATH = \"drive/My Drive/BCI_clone/npy/npy_0_label.npy\"\n",
        "\n",
        "ALL_SUBJECTS_NPY_DATA_PATH = \"drive/My Drive/BCI_clone/npy/npy_all_subjects.npy\"\n",
        "ALL_SUBJECTS_NPY_LABEL_PATH = \"drive/My Drive/BCI_clone/npy/npy_all_subjects_label.npy\"\n",
        "\n",
        "batch_size = 32\n",
        "num_nodes = 62\n",
        "num_features = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wRjxshuVMIb"
      },
      "source": [
        "# computed in preprocess\n",
        "min_val = 10.567626836074302\n",
        "max_val = 42.11366999020901"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBHSza5BXXG"
      },
      "source": [
        "Build the model input (batched graphs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0YtOKxwxa64"
      },
      "source": [
        "def get_train_data(X, Y, batch_size=32, drop_incomplete=True):\n",
        "  X_batched, Y_batched = [], []\n",
        "  num_graphs = X.shape[0]\n",
        "  \n",
        "  for i in range(0, num_graphs, batch_size):\n",
        "    if drop_incomplete is True and (i + batch_size > X.shape[0]): break;\n",
        "    \n",
        "    batch = X[i:i+batch_size]\n",
        "    assert (batch.shape == (batch_size, num_nodes, num_features))\n",
        "    batch = tf.constant(np.reshape(batch, (-1, num_features)))\n",
        "    \n",
        "    assert (batch.shape == (batch_size*num_nodes, num_features))\n",
        "    assert (tf.math.reduce_min(batch) >= min_val)\n",
        "    assert (tf.math.reduce_max(batch) <= max_val) \n",
        "\n",
        "    X_batched.append(batch)\n",
        "    Y_batched.append(tf.constant(Y[i:i+batch_size], dtype=\"float32\"))\n",
        "\n",
        "  X_batched = tf.stack(X_batched, axis=0)\n",
        "  Y_batched = tf.stack(Y_batched, axis=0)\n",
        "  assert (X_batched.shape == (num_graphs//batch_size, batch_size*num_nodes, num_features))\n",
        "  assert (Y_batched.shape == (num_graphs//batch_size, batch_size,))\n",
        "\n",
        "  return X_batched, Y_batched"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVUOJGH_a8zU"
      },
      "source": [
        "def emotion_dl(batched_labels:list, noise:int=0.18):\n",
        "  d_labels = []\n",
        "\n",
        "  def get_distrib(label:int):\n",
        "    if label == 0: return tf.constant([1-2*noise/3, noise/3, 0.0], dtype=\"float32\")\n",
        "    if label == 1: return tf.constant([2*noise/3, 1-2*noise/3, 2*noise/3], dtype=\"float32\")\n",
        "    if label == 2: return tf.constant([0.0, noise/3, 1-2*noise/3], dtype=\"float32\")\n",
        "  \n",
        "  for batch in batched_labels:\n",
        "    batch = tf.map_fn(lambda t: get_distrib(int(t)), batch)\n",
        "    assert (batch.shape == (batch_size, 3))\n",
        "    d_labels.append(batch)\n",
        "\n",
        "  assert (len(d_labels) == len(batched_labels))\n",
        "  return d_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SGw36tTBWfv",
        "outputId": "2be93ce0-9737-4afc-922c-65b9162671eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data_path = SUBJECT_NPY_DATA_PATH\n",
        "labels_path = SUBJECT_NPY_LABEL_PATH\n",
        "X = np.load(data_path)\n",
        "Y = np.load(labels_path)\n",
        "print(X.shape, Y.shape) #num_graphs x num_ch x num_features\n",
        "X, Y = get_train_data(X, Y)\n",
        "X, Y = tf.unstack(X), tf.unstack(Y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=42)\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "# get label distributions\n",
        "y_train, y_test = emotion_dl(y_train), emotion_dl(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2025, 62, 25) (2025,)\n",
            "31\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ30UlkJQheC"
      },
      "source": [
        "num_batches = min(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMTV6jSfOIcA"
      },
      "source": [
        "# initialize the adjacency matrix A\n",
        "# to what extent would help to use a sparse matrix instead?\n",
        "# A[i][j] = exp(-dist[i][j] / 2*delta^2)\n",
        "# delta = 0.1\n",
        "# dist[i][j] - the physical distance between the ith and jth channels\n",
        "def build_adj_matrix(dist):\n",
        "  d_shape = dist.shape\n",
        "  assert (d_shape == (num_nodes,num_nodes))\n",
        "  A = np.zeros(shape=(num_nodes*batch_size,num_nodes*batch_size))\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    for x in range(d_shape[0]):\n",
        "      for y in range(d_shape[1]):\n",
        "            A[i*d_shape[0]+x][i*d_shape[1]+y] = dist[x][y]\n",
        "  # np.save(os.path.join(CSV_PATH, \"adj_matrix.npy\"), A)\n",
        "  A = tf.convert_to_tensor(A, dtype=\"float32\")\n",
        "  return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwuGxsdPr3Xw"
      },
      "source": [
        "class GCNLayer(tf.keras.layers.Layer):\n",
        "  '''\n",
        "  '''\n",
        "  def __init__(self, hidden_dim=32, activation=tf.keras.activations.relu, \n",
        "               activation_att=tf.keras.activations.tanh, pooling_ratio=1):\n",
        "    super(GCNLayer, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.activation = activation\n",
        "    self.activation_att = activation_att\n",
        "    self.weight_init = tf.keras.initializers.GlorotNormal()\n",
        "    self.pooling_ratio = pooling_ratio #(0, 1], 1 no pooling\n",
        "\n",
        "  def get_degree_matrix(self, A:tf.Tensor):\n",
        "    \"sum across second dimension and transform the reduced matrix into eye\"\n",
        "    diagonals = tf.math.reduce_sum(A, axis=1)\n",
        "    return tf.linalg.tensor_diag(diagonals)\n",
        "                                 \n",
        "  def get_norm_laplacian(self, A:tf.Tensor):\n",
        "    \"\"\" Compute L = (D**-1/2) * A * (D**-1/2)\n",
        "        Make sure A contains self-loops.\n",
        "    \"\"\"\n",
        "    D = self.get_degree_matrix(A)\n",
        "    D = tf.math.pow(D, -0.5)\n",
        "    D = tf.where(tf.math.is_inf(D), tf.zeros_like(D), D) # replace inf values with 0\n",
        "    L = tf.matmul(D, A)\n",
        "    L = tf.matmul(L, D)\n",
        "    return L\n",
        "  \n",
        "  def topk_batch(self, X, k):\n",
        "    # reshape to [batch_size, num_nodes]\n",
        "    X = tf.reshape(X, [batch_size, -1])\n",
        "    \n",
        "    # get the topk values from each batch\n",
        "    topk_values, topk_indices = tf.math.top_k(X, k, sorted=False)\n",
        "\n",
        "    # add batch no offset \n",
        "    offset = tf.constant([[sample]*k for sample in range(batch_size)])\n",
        "    topk_indices = tf.math.add(topk_indices, offset)\n",
        "\n",
        "    # flatten to [k*batch_size]\n",
        "    topk_indices = tf.reshape(topk_indices, [-1])\n",
        "    topk_values = tf.reshape(topk_values, [-1])\n",
        "    \n",
        "    assert(topk_indices.shape == (batch_size*k,))\n",
        "    assert(topk_values.shape == (batch_size*k,))\n",
        "\n",
        "    return topk_values, topk_indices\n",
        "  \n",
        "  def pool(self, X:tf.Tensor, A):\n",
        "    # get the self-attention score matrix: X_att = X * W_att\n",
        "    X_att = self.activation_att(tf.matmul(X, self.W_att)) #shape(N, 1)\n",
        "    \n",
        "    # get the nodes with topk attention scores from \n",
        "    # each batch; add a treshold to k to be sure k != 0\n",
        "    k = max(int(np.ceil(self.num_nodes * self.pooling_ratio)), 20)\n",
        "    topk_values, topk_indices = self.topk_batch(X_att, k)\n",
        "    \n",
        "    # apply pooling to node features X = X[topk_indices] * topk_values\n",
        "    X = tf.gather(X, topk_indices)\n",
        "    X = tf.transpose(X)\n",
        "    X = tf.math.multiply(X, topk_values)\n",
        "    X = tf.transpose(X)\n",
        "\n",
        "    # filter the adjacency matrix to account for the selected nodes only\n",
        "    A = tf.gather(A, topk_indices, axis=0)\n",
        "    A = tf.gather(A, topk_indices, axis=1)\n",
        "\n",
        "    assert(X.shape==(batch_size*k, self.hidden_dim))\n",
        "    assert(A.shape==(batch_size*k, batch_size*k))\n",
        "\n",
        "    return X, A\n",
        "\n",
        "  def readout(self, X:tf.Tensor):\n",
        "    \"\"\" Readout function to collapse all nodes into a single representation\"\"\"\n",
        "    X = tf.reshape(X, [batch_size,-1,self.hidden_dim])\n",
        "\n",
        "    global_mean_pool = tf.math.reduce_mean(X, axis=1)\n",
        "    global_max_pool = tf.math.reduce_max(X, axis=1)\n",
        "\n",
        "    assert(global_mean_pool.shape == (batch_size, self.hidden_dim))\n",
        "    assert(global_max_pool.shape == (batch_size, self.hidden_dim))\n",
        "\n",
        "    return tf.concat([global_mean_pool, global_max_pool], axis=1)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.num_nodes = int(input_shape.as_list()[0]) / batch_size\n",
        "    num_features = input_shape[-1]\n",
        "    self.W = self.add_weight(\"W\", shape=[num_features, self.hidden_dim], initializer=self.weight_init)\n",
        "    self.W_att = self.add_weight(\"W_att\", shape=[self.hidden_dim, 1], initializer=self.weight_init)\n",
        "\n",
        "  def call(self, X:tf.Tensor, A:tf.Tensor):\n",
        "    ''' \n",
        "      Compute graph convolution with self-attention poosing.\n",
        "      Returns:\n",
        "        - X: result from graph convolution only\n",
        "        - X_pooled: result after pooling layer\n",
        "        - red: readout output\n",
        "        - A: the adjacency matrix of the pooled graph\n",
        "    '''\n",
        "    L = self.get_norm_laplacian(A) #L = D^(-1/2) * A * D^(-1/2)\n",
        "    X = tf.matmul(L, X) # graph convolution: X = L * X\n",
        "    X = self.activation(tf.matmul(X, self.W)) #linear embedding: X = sigma(X*W)\n",
        "    X_pooled, A = self.pool(X, A)  # graph pooling\n",
        "    red = self.readout(X_pooled) # readout and concatennate\n",
        "    \n",
        "    tf.debugging.assert_all_finite(X, \"not finite\")\n",
        "    tf.debugging.assert_all_finite(X_pooled, \"not finite\")\n",
        "    tf.debugging.assert_all_finite(red, \"not finite\")\n",
        "    tf.debugging.assert_all_finite(A, \"not finite\")\n",
        "    \n",
        "    return X, X_pooled, red, A\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9LeyDbt3w7V"
      },
      "source": [
        "def init_adj():\n",
        "  initializer = tf.keras.initializers.RandomUniform(0.5, 0.9)\n",
        "  values = initializer(shape=(62, 62))\n",
        "  # replace above with between channels distances\n",
        "  diagonal = np.array([1]*num_nodes)\n",
        "  values = tf.linalg.set_diag(values, diagonal)\n",
        "  A = build_adj_matrix(values)\n",
        "  A = tf.Variable(A, trainable=True, name=\"A\")\n",
        "  return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EINgtHfejLct"
      },
      "source": [
        "class RGNN(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, out_dim, hidden_dim, A_init=None):\n",
        "    super(RGNN, self).__init__()\n",
        "    # hyper-parameters\n",
        "    self.num_blocks = num_blocks\n",
        "    self.out_dim = out_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    \n",
        "    # the adjacency matrix parameter\n",
        "    initializer = tf.keras.initializers.GlorotNormal()\n",
        "    values = initializer(shape=(32*62, 32*62))\n",
        "    self.A = init_adj()\n",
        "\n",
        "    self.blocks = [GCNLayer(hidden_dim=hidden_dim, pooling_ratio=0.8) for i in range(num_blocks)]\n",
        "    # output layers\n",
        "    self.graph_output = tf.keras.layers.Dense(self.out_dim, activation=\"softmax\", name=\"target\")\n",
        "    self.domain_output = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"domain\")\n",
        "  \n",
        "  def call(self, train, test=None):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "      - out:tf.Tensor(batch_size, out_dim) - graph-level emotion probabilities\n",
        "      - domain_train:tf.Tensor(batch_size*num_pooled_nodes, 2)\n",
        "      - domain_test:tf.Tensor(batch_size*num_pooled_nodes, 2)\n",
        "    \"\"\"\n",
        "    adjS = self.A; adjT = self.A\n",
        "    redS = []\n",
        "    convS, pooledS, red, adjS = self.blocks[0](train, adjS)\n",
        "    convT, pooledT, redT, adjT = self.blocks[0](test, adjT)\n",
        "    redS.append(red)\n",
        "    \n",
        "    for i in range(1, self.num_blocks):\n",
        "      convS, pooledS, red, adjS = self.blocks[i](pooledS, adjS)\n",
        "      redS.append(red)\n",
        "      convT, pooledT, redT, adjT = self.blocks[i](pooledT, adjT)\n",
        "    \n",
        "    assert (len(redS) == len(self.blocks))\n",
        "    # add the readouts from the source domain\n",
        "    concat_redS = tf.math.add_n(redS)\n",
        "    \n",
        "    # node-wise domain classification for finer granularity\n",
        "    domain_train = self.domain_output(convS)\n",
        "    domain_test = self.domain_output(convT)\n",
        "    \n",
        "    # target task classification logits\n",
        "    out = self.graph_output(concat_redS)\n",
        "    return out, domain_train, domain_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAkFK2YtMsRd"
      },
      "source": [
        "class Trainer(object):\n",
        "  \"\"\"\n",
        "  Class to train model in EEG classification\n",
        "  \"\"\"\n",
        "  def __init__(self, num_epochs=200, lr=1e-3):\n",
        "    self.num_epochs = num_epochs\n",
        "    self.kl = tf.keras.losses.KLDivergence()\n",
        "    self.dl = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "    self.beta = 0\n",
        "    self.model = RGNN(num_blocks=3, out_dim=3, hidden_dim=64)\n",
        "\n",
        "  def run(self):\n",
        "    for epoch in range(self.num_epochs):\n",
        "      epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "      for i in range(num_batches):\n",
        "        graph_train = X_train[i]\n",
        "        graph_test = X_test[i]\n",
        "        labels_train = tf.constant(y_train[i])\n",
        "        \n",
        "        assert (graph_train.shape == (batch_size*num_nodes, num_features))\n",
        "        assert (graph_test.shape == (batch_size*num_nodes, num_features))\n",
        "        assert (labels_train.shape == (batch_size, 3))\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "          graph_preds, domain_train, domain_test = self.model(graph_train, graph_test)\n",
        "          domain_preds = tf.concat([domain_train, domain_test], axis=0)\n",
        "          domain_labels = tf.concat([tf.convert_to_tensor([0]*domain_train.shape[0]),\n",
        "                                     tf.convert_to_tensor([1]*domain_test.shape[0])], axis=0) \n",
        "          assert (domain_preds.shape == (2*domain_train.shape[0], 2))\n",
        "          assert (domain_labels.shape == (domain_preds.shape[0],))\n",
        "      \n",
        "          target_loss = self.kl(labels_train, graph_preds)\n",
        "          domain_loss = self.dl(domain_labels, domain_preds)\n",
        "      \n",
        "        # params order: gcn1/W, gcn1/W_att, gcn2/W, gcn2/W_att, gcn3/W, gcn3/W_att, W_out, bias_out W_domain, bias_domain, A\n",
        "        print(f'Loss: {target_loss}')\n",
        "        params = self.model.trainable_variables\n",
        "        # print(len(params))\n",
        "        # for param in params:\n",
        "        #   print(param.name)\n",
        "        # print(f'domain_loss:{domain_loss}')\n",
        "        \n",
        "        target_grads = tape.gradient(target_loss, params[0:-3]) + [tape.gradient(target_loss, params[-1])]\n",
        "        reverse_grads = tape.gradient(domain_loss, params[0:-6]) + [tape.gradient(domain_loss, params[-1])]\n",
        "        domain_grads = tape.gradient(domain_loss, params[-3:-1]) \n",
        "\n",
        "        for i, grad in enumerate(reverse_grads): \n",
        "          if grad is None: print(i)\n",
        "\n",
        "        for grad in target_grads: tf.debugging.assert_all_finite(grad, \"not finite target grad\")\n",
        "        for grad in reverse_grads: tf.debugging.assert_all_finite(grad, \"not finite reverse grad\")\n",
        "        for grad in domain_grads: tf.debugging.assert_all_finite(grad, \"not finite domain grad\")\n",
        "        \n",
        "        # minimize the KL loss for the target task\n",
        "        self.optimizer.apply_gradients(zip(target_grads, params[0:-3]+[params[-1]]))\n",
        "        \n",
        "        # minimize the domain loss with respect to the params of the domain_output layer\n",
        "        self.optimizer.apply_gradients(zip(domain_grads, params[-3:-1]))\n",
        "\n",
        "        # negate the gradients and multiply by annealed factor beta\n",
        "        reverse_grads = [tf.math.negative(reverse_grads[i]) for i in range(len(reverse_grads))]\n",
        "        progress = (epoch+1) / self.num_epochs\n",
        "        self.beta = 2/(1 + np.exp(-10*progress)) - 1\n",
        "        reverse_grads = [tf.math.scalar_mul(self.beta, reverse_grads[i]) for i in range(len(reverse_grads))] \n",
        "        \n",
        "        # maximize the domain loss with respect to the rest of params\n",
        "        self.optimizer.apply_gradients(zip(reverse_grads, params[0:-6]+[params[-1]]))\n",
        "        epoch_loss_avg.update_state(target_loss)\n",
        "\n",
        "      print(\"Epoch: {:03d}, Loss: {:.3f}\". format(epoch, epoch_loss_avg.result()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1_uoGaJN_HM"
      },
      "source": [
        "trainer = Trainer()\n",
        "trainer.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}